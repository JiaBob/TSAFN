{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim \n",
    "\n",
    "from PIL import Image\n",
    "import glob, re, os, copy, random, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TSAFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TSAFN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv2d(5, 64, 7, 1, 3),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                    nn.Conv2d(64, 32, 5, 1, 2),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                    nn.Conv2d(32, 16, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(16),\n",
    "                    nn.ReLU(True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                    nn.Conv2d(16, 3, 5, 1, 2),\n",
    "                    nn.BatchNorm2d(3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return x.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TPN, self).__init__()\n",
    "        self.conv1_1 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv1_3 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv1_4 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_1 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_2 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_3 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_4 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_1 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_2 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_3 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_4 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 1, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(1),\n",
    "                      nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        N, C, h, w = x.size()\n",
    "        \n",
    "        size1 = x\n",
    "        size2 = F.interpolate(x, size=(h//2, w//2))\n",
    "        size4 = F.interpolate(x, size=(h//4, w//4))\n",
    "        size8 = F.interpolate(x, size=(h//8, w//8))\n",
    "        \n",
    "        size1 = self.conv1_1(size1)\n",
    "        size2 = self.conv1_2(size2)\n",
    "        size4 = self.conv1_3(size4)\n",
    "        size8 = self.conv1_4(size8)\n",
    "        \n",
    "        size1 = self.conv2_1(size1)\n",
    "        size2 = self.conv2_2(size2)\n",
    "        size4 = self.conv2_3(size4)\n",
    "        size8 = self.conv2_4(size8)\n",
    "        \n",
    "        size1 = self.conv3_1(size1)\n",
    "        size2 = self.conv3_2(size2)\n",
    "        size4 = self.conv3_3(size4)\n",
    "        size8 = self.conv3_4(size8)\n",
    "        \n",
    "        size2 = F.interpolate(size2, size=(h,w))\n",
    "        size4 = F.interpolate(size4, size=(h,w))\n",
    "        size8 = F.interpolate(size8, size=(h,w))\n",
    "        \n",
    "        concat = torch.cat((size1, size2, size4, size8), 1)\n",
    "        \n",
    "        return self.conv4(concat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothData(Dataset):\n",
    "    def __init__(self, path, split_ratio={'train':0.65, 'val':0.1, 'test':0.25}, transform=None):\n",
    "        self.transforms = transform\n",
    "        self.category = {}\n",
    "        self.name = {'input', 'target_SPN', 'target_TPN', 'target_TSAFN'}\n",
    "        self.mode = None\n",
    "        self.modeset = ['train', 'val', 'test']\n",
    "        \n",
    "        check_folder = set()\n",
    "        for directory, folders, _ in os.walk(path):\n",
    "            for f in folders:\n",
    "                self.category[f] = glob.glob(os.path.join(directory, f) + '\\\\*')\n",
    "                check_folder.add(f)\n",
    "            break  # only retrieve one level\n",
    "            \n",
    "        assert not (self.name - check_folder), 'folder name must be same as declared in self.name'\n",
    "        assert sum(split_ratio.values()) == 1, 'the summation of values in split_ratio must be one'\n",
    "\n",
    "        self.total = len(self.category['input'])\n",
    "        \n",
    "        index_shuffle = list(range(self.total))\n",
    "        random.shuffle(index_shuffle)\n",
    "        \n",
    "        train_amount = int(self.total * split_ratio['train'])\n",
    "        val_amount = int(self.total * split_ratio['val'])\n",
    "        self.amount = {'train': train_amount,\n",
    "                       'val': val_amount,\n",
    "                       'test': self.total - train_amount - val_amount}  # must use minus, otherwise not full.         \n",
    "\n",
    "        index_train = index_shuffle[: self.amount['train']]\n",
    "        index_val = index_shuffle[self.amount['train'] : self.amount['train'] + self.amount['val']]\n",
    "        index_test = index_shuffle[self.amount['train'] + self.amount['val'] :]\n",
    "\n",
    "        \n",
    "        # the file retrieval order is not sequentially from 1 to end, but may be like 1, 10, 11, ... , 2, 20 ..\n",
    "        # this part is to align the image index into sequential order for target_SPN and target_TSAFN\n",
    "        target_length = len(self.category['target_SPN'])\n",
    "        temp1 = [0 for i in range(target_length)]\n",
    "        temp2 = copy.deepcopy(temp1)\n",
    "        for i in range(target_length):\n",
    "            img_path = self.category['target_SPN'][i]\n",
    "            _, img_name = os.path.split(img_path)\n",
    "            img_index = int(img_name.split('.')[0])\n",
    "            temp1[img_index] = img_path\n",
    "            \n",
    "            img_path = self.category['target_TSAFN'][i]\n",
    "            _, img_name = os.path.split(img_path)\n",
    "            img_index = int(img_name.split('.')[0])\n",
    "            temp2[img_index] = img_path\n",
    "        self.category['target_SPN'] = temp1\n",
    "        self.category['target_TSAFN'] = temp2\n",
    "        \n",
    "        self.collection = {'train':([[0 for i in range(4)] for i in range(self.amount['train'])], index_train),\n",
    "                           'val':([[0 for i in range(4)] for i in range(self.amount['val'])], index_val),\n",
    "                           'test':([[0 for i in range(4)] for i in range(self.amount['test'])], index_test)}\n",
    "        \n",
    "        for m in self.modeset:\n",
    "            for subset_index, alldata_index in enumerate(self.collection[m][1]):\n",
    "                _, img_name = os.path.split(self.category['input'][alldata_index])\n",
    "                img_index = int(img_name.split('_')[0])\n",
    "\n",
    "                self.collection[m][0][subset_index] = [self.category['input'][alldata_index], \n",
    "                                      self.category['target_TPN'][alldata_index], \n",
    "                                      self.category['target_SPN'][img_index], \n",
    "                                      self.category['target_TSAFN'][img_index]]  \n",
    "\n",
    "    def __call__(self, mode):\n",
    "        assert mode in self.modeset, 'mode must be either train, val or test'\n",
    "        self.mode = mode\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.mode:\n",
    "            return self.amount[self.mode]\n",
    "        else:\n",
    "            raise Exception('Firstly, you must use self.setmode(mode) to set which data set (train, val, test) to be use.')\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        # check if mode is set\n",
    "        len(self)\n",
    "       \n",
    "        dataset = self.collection[self.mode][0]\n",
    "        inpu = Image.open(dataset[i][0])\n",
    "        target_TPN = Image.open(dataset[i][1])\n",
    "        target_SPN = Image.open(dataset[i][2])\n",
    "        target_TSAFN = Image.open(dataset[i][3])\n",
    "        \n",
    "        if self.transforms:\n",
    "            inpu = self.transforms(inpu)\n",
    "            target_TPN = self.transforms(target_TPN)\n",
    "            target_SPN = self.transforms(target_SPN)\n",
    "            target_TSAFN = self.transforms(target_TSAFN)\n",
    "\n",
    "        return {'input':inpu, 'TPN':target_TPN, 'SPN':target_SPN, 'TSAFN':target_TSAFN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPN(nn.Module):\n",
    "    def __init__(self, body_model='vgg11'):\n",
    "        super(SPN, self).__init__()\n",
    "        \n",
    "        self.body = VGGNet(model=body_model)\n",
    "        \n",
    "        self.side_output_layer1 = nn.Conv2d(64, 1, 1)\n",
    "        self.side_output_layer2 = nn.Conv2d(128, 1, 1)\n",
    "        self.side_output_layer3 = nn.Conv2d(256, 1, 1)\n",
    "        self.side_output_layer4 = nn.Conv2d(512, 1, 1)\n",
    "        self.side_output_layer5 = nn.Conv2d(512, 1, 1)\n",
    "        \n",
    "        self.fusion = nn.Conv2d(5, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, h, w = x.size()\n",
    "        \n",
    "        # load vgg\n",
    "        body = self.body(x)\n",
    "\n",
    "        # side-output layers\n",
    "        side_output1 = body['side_output1']\n",
    "        side_output1 = self.side_output_layer1(side_output1)\n",
    "        side_output1 = F.interpolate(side_output1, size=(h, w))\n",
    "        \n",
    "        side_output2 = body['side_output2']\n",
    "        side_output2 = self.side_output_layer2(side_output2)\n",
    "        side_output2 = F.interpolate(side_output2, size=(h,w))\n",
    "        \n",
    "        side_output3 = body['side_output3']\n",
    "        side_output3 = self.side_output_layer3(side_output3)\n",
    "        side_output3 = F.interpolate(side_output3, size=(h,w))\n",
    "        \n",
    "        side_output4 = body['side_output4']\n",
    "        side_output4 = self.side_output_layer4(side_output4)\n",
    "        side_output4 = F.interpolate(side_output4, size=(h,w))\n",
    "        \n",
    "        side_output5 = body['side_output5']\n",
    "        side_output5 = self.side_output_layer5(side_output5)\n",
    "        side_output5 = F.interpolate(side_output5, size=(h,w))\n",
    "        \n",
    "        # fusion layer\n",
    "        fuse = torch.cat((side_output1, side_output2, side_output3, side_output4, side_output5), 1)\n",
    "        fusion = self.fusion(fuse)\n",
    "\n",
    "        # will use cross_entropy_with_logit loss, so no need apply sigmoid on the final output.\n",
    "        return side_output1, side_output2, side_output3, side_output4, side_output5, fusion\n",
    "\n",
    "\n",
    "# the strategy to both get the structure and pretrained weight of in-built vgg\n",
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg11', requires_grad=True):\n",
    "  \n",
    "        super().__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.{}_bn(pretrained=True).state_dict())\".format(model))\n",
    "        \n",
    "        # for fix weight transfor learning\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # first reconstructure same vgg structure as in-bulit to utilize pretraining, but del the redundent fc later\n",
    "        del self.classifier  \n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # catch the middle outputs \n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            \n",
    "            output[\"side_output%d\"%(idx+1)] = x\n",
    "\n",
    "        return output\n",
    "\n",
    "# this range only for batchnorm version\n",
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 7),  (7, 14),  (14, 21), (21, 28)),\n",
    "    'vgg13': ((0, 6), (6, 13), (13, 20), (20, 27), (27, 34)),\n",
    "    'vgg16': ((0, 6), (6, 13), (13, 23), (23, 32), (32, 43)),\n",
    "    'vgg19': ((0, 6), (6, 13), (13, 26), (26, 39), (39, 52))\n",
    "}\n",
    "\n",
    "def make_layers(cfg):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPN_one_iter(model, data_loader, optimizer=None):\n",
    "    assert model.__class__.__name__ == 'TPN', 'this TPN_one_iter only works for TPN model' \n",
    "    \n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "        \n",
    "    loss_sum = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            out = model(x['input'].to(device))\n",
    "            loss = criterion(out, x['TPN'].to(device)).to(device)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def weighted_cross_entropy(pred, target):\n",
    "    total = target.numel()  # it is int\n",
    "    negative = (target == 0).sum().item()  # it is tensor before call .item() \n",
    "    \n",
    "    mask = torch.empty_like(target, dtype=torch.float)\n",
    "    mask[target == 0] = 1 - negative / total\n",
    "    mask[target != 0] = negative / total\n",
    "    \n",
    "    # If rigorously follow what the HED paper did, argument reduction='sum' should be added. \n",
    "    # However, to make all losses under small scale, we remove it. Same for BCEWithLogitsLoss below. \n",
    "    return F.binary_cross_entropy_with_logits(pred, target, mask)\n",
    "\n",
    "\n",
    "def SPN_loss(so1, so2, so3, so4, so5, fusion, target):\n",
    "    # non-weighted BCE\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(fusion, target) \n",
    "\n",
    "    # there are weights called alpha mentioned in paper which is used to combine \n",
    "    # the side output loss summation. However, no further description about alpha\n",
    "    # thus here just simply sum them together without weights.\n",
    "\n",
    "    loss += weighted_cross_entropy(so1, target)\n",
    "    loss += weighted_cross_entropy(so2, target)\n",
    "    loss += weighted_cross_entropy(so3, target)\n",
    "    loss += weighted_cross_entropy(so4, target)\n",
    "    loss += weighted_cross_entropy(so5, target)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def SPN_one_iter(model, data_loader, optimizer=None):\n",
    "    assert model.__class__.__name__ == 'SPN', 'this SPN_one_iter only works for SPN model' \n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "    \n",
    "    loss_sum = 0\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            output = model(x['input'].to(device))\n",
    "            target = x['TPN'].to(device)\n",
    "            \n",
    "            loss = SPN_loss(*output, target).to(device)\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "        \n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def TSAFN_one_iter(model, data_loader, optimizer=None):\n",
    "    assert model.__class__.__name__ == 'TSAFN', 'this TSAFN_one_iter only works for TSAFN model' \n",
    "    \n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss_sum = 0\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            inpu = torch.cat((x['input'].to(device), x['TPN'].to(device), x['SPN'].to(device)), 1)\n",
    "            out = model(inpu)\n",
    "            loss = criterion(out, x['TSAFN'].to(device)).to(device)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def three_to_one(model_dict, data_loader, optimizer=None):\n",
    "    TPN = model_dict['TPN']\n",
    "    SPN = model_dict['SPN']\n",
    "    TSAFN = model_dict['TSAFN']\n",
    "\n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss_sum = 0\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            inpu = x['input'].to(device)\n",
    "            target_TPN = x['TPN'].to(device)\n",
    "            target_SPN = x['SPN'].to(device)\n",
    "            target_TSAFN = x['TSAFN'].to(device)\n",
    "            \n",
    "            output_TPN = TPN(inpu)\n",
    "            output_SPN = SPN(inpu)\n",
    "            concat = torch.cat((inpu, output_TPN, output_SPN[-1]), 1) # what really counts for SPN is fusion output \n",
    "            out = TSAFN(concat)\n",
    "            \n",
    "            loss_SPN = SPN_loss(*output_SPN, target_SPN)\n",
    "            loss_TPN = F.mse_loss(output_TPN, target_TPN)\n",
    "            loss_TSAFN = F.mse_loss(out, target_TSAFN)\n",
    "            print(loss_SPN)\n",
    "            print(loss_TPN)\n",
    "            print(loss_TSAFN)\n",
    "            loss = 0.6 * loss_TSAFN + 0.2 * (loss_SPN + loss_TPN)\n",
    "            loss = loss.to(device)\n",
    "            print(loss)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "    \n",
    "\n",
    "def train(model, model_iter, train_loader, val_loader, optimizer=None, epochs=500, save_interval=2):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_time = time.time()\n",
    "        train_loss = model_iter(model, train_loader, optimizer)\n",
    "        val_loss = model_iter(model, val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_time\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(\"Epoch {} finished, current loss is {} takes {}\".format(epoch + 1, val_loss, \n",
    "                                                                          timeformat(epoch_time)))\n",
    "            print('It takes {} from beginning'.format(timeformat(time.time() - start_time)))\n",
    "            \n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            torch.save(model, '{}epoch_result'.format(epoch + 1))\n",
    "        \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def timeformat(s):\n",
    "    s = int(s)\n",
    "    m, s = divmod(s, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    t = '{:>02d}:{:>02d}:{:>02d}'.format(h, m, s) if h else '{:>02d}:{:>02d}'.format(m, s) if m else '{:2d}s'.format(s)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "data = SmoothData('../verysmall_dataset', transform=transform)  \n",
    "\n",
    "#train_sampler = sampler.SubsetRandomSampler(range(18000))\n",
    "#val_sampler = sampler.SubsetRandomSampler(range(18000, 20000))\n",
    "\n",
    "train_loader = DataLoader(data('train'), batch_size=2)\n",
    "val_loader = DataLoader(data('val'), batch_size=10)\n",
    "test_loader = DataLoader(data('test'), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2999, grad_fn=<ThAddBackward>)\n",
      "tensor(0.1707, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1392, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3776, grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-670-bff527675e2b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, model_iter, train_loader, val_loader, optimizer, epochs, save_interval)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0mepoch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-670-bff527675e2b>\u001b[0m in \u001b[0;36mthree_to_one\u001b[1;34m(model_dict, data_loader, optimizer)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming_tools\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming_tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dict = {'SPN': SPN().to(device), 'TPN': TPN().to(device), 'TSAFN': TSAFN().to(device)} \n",
    "#model = torch.load('9epoch_result')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "loss, val_losses = train(model_dict, three_to_one, train_loader, val_loader, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-c9a5400bffed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSmoothData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\smooth_net\\data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#c = utils.make_grid(torch.cat((a[0], a[1], a[2], a[3]), 2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-285cd051f099>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mtarget_TSAFN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_TPN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_TPN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "a = SmoothData('D:\\smooth_net\\data', transform=transform)[3]\n",
    "#c = utils.make_grid(torch.cat((a[0], a[1], a[2], a[3]), 2))\n",
    "c1 = a[0].numpy().transpose((1,2,0))\n",
    "c2 = a[1].numpy().transpose((1,2,0))\n",
    "c3 = a[2].numpy().transpose((1,2,0))\n",
    "c4 = a[3].numpy().transpose((1,2,0))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(141)\n",
    "plt.imshow(c1)\n",
    "plt.subplot(142)\n",
    "plt.imshow(c2[:,:,0], cmap='gray')\n",
    "plt.subplot(143)\n",
    "plt.imshow(c3[:,:,0], cmap='gray')\n",
    "plt.subplot(144)\n",
    "plt.imshow(c4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
