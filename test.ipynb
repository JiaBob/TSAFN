{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim \n",
    "\n",
    "from PIL import Image\n",
    "import glob, re, os, copy, random, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TSAFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TSAFN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv2d(5, 64, 7, 1, 3),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                    nn.Conv2d(64, 32, 5, 1, 2),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                    nn.Conv2d(32, 16, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(16),\n",
    "                    nn.ReLU(True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                    nn.Conv2d(16, 3, 5, 1, 2),\n",
    "                    nn.BatchNorm2d(3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return x.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TPN, self).__init__()\n",
    "        self.conv1_1 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv1_3 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv1_4 = nn.Sequential(\n",
    "                      nn.Conv2d(3, 16, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(16),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_1 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_2 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_3 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2_4 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 8, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(8),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_1 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_2 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_3 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3_4 = nn.Sequential(\n",
    "                      nn.Conv2d(8, 4, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(4),\n",
    "                      nn.ReLU(True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                      nn.Conv2d(16, 1, 3, 1, 1),\n",
    "                      nn.BatchNorm2d(1),\n",
    "                      nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        N, C, h, w = x.size()\n",
    "        \n",
    "        size1 = x\n",
    "        size2 = F.interpolate(x, size=(h//2, w//2))\n",
    "        size4 = F.interpolate(x, size=(h//4, w//4))\n",
    "        size8 = F.interpolate(x, size=(h//8, w//8))\n",
    "        \n",
    "        size1 = self.conv1_1(size1)\n",
    "        size2 = self.conv1_2(size2)\n",
    "        size4 = self.conv1_3(size4)\n",
    "        size8 = self.conv1_4(size8)\n",
    "        \n",
    "        size1 = self.conv2_1(size1)\n",
    "        size2 = self.conv2_2(size2)\n",
    "        size4 = self.conv2_3(size4)\n",
    "        size8 = self.conv2_4(size8)\n",
    "        \n",
    "        size1 = self.conv3_1(size1)\n",
    "        size2 = self.conv3_2(size2)\n",
    "        size4 = self.conv3_3(size4)\n",
    "        size8 = self.conv3_4(size8)\n",
    "        \n",
    "        size2 = F.interpolate(size2, size=(h,w))\n",
    "        size4 = F.interpolate(size4, size=(h,w))\n",
    "        size8 = F.interpolate(size8, size=(h,w))\n",
    "        \n",
    "        concat = torch.cat((size1, size2, size4, size8), 1)\n",
    "        \n",
    "        return self.conv4(concat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TPN_one_iter(model, data_loader, optimizer=None):\n",
    "    assert model.__class__.__name__ == 'TPN', 'this TPN_one_iter only works for TPN model' \n",
    "    \n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "        \n",
    "    loss_sum = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            out = model(x['input'].to(device))\n",
    "            loss = criterion(out, x['TPN'].to(device)).to(device)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def weighted_cross_entropy(pred, target):\n",
    "    total = target.numel()  # it is int\n",
    "    negative = (target == 0).sum().item()  # it is tensor before call .item() \n",
    "    \n",
    "    mask = torch.empty_like(target, dtype=torch.float)\n",
    "    mask[target == 0] = 1 - negative / total\n",
    "    mask[target != 0] = negative / total\n",
    "    \n",
    "    # If rigorously follow what the HED paper did, argument reduction='sum' should be added. \n",
    "    # However, to make all losses under small scale, we remove it. Same for BCEWithLogitsLoss below. \n",
    "    return F.binary_cross_entropy_with_logits(pred, target, mask)\n",
    "\n",
    "\n",
    "def SPN_loss(so1, so2, so3, so4, so5, fusion, target):\n",
    "    # non-weighted BCE\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(fusion, target) \n",
    "\n",
    "    # there are weights called alpha mentioned in paper which is used to combine \n",
    "    # the side output loss summation. However, no further description about alpha\n",
    "    # thus here just simply sum them together without weights.\n",
    "\n",
    "    loss += weighted_cross_entropy(so1, target)\n",
    "    loss += weighted_cross_entropy(so2, target)\n",
    "    loss += weighted_cross_entropy(so3, target)\n",
    "    loss += weighted_cross_entropy(so4, target)\n",
    "    loss += weighted_cross_entropy(so5, target)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def SPN_one_iter(model, data_loader, optimizer=None):\n",
    "    assert model.__class__.__name__ == 'SPN', 'this SPN_one_iter only works for SPN model' \n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "    \n",
    "    loss_sum = 0\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            output = model(x['input'].to(device))\n",
    "            target = x['TPN'].to(device)\n",
    "            \n",
    "            loss = SPN_loss(*output, target).to(device)\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "        \n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def TSAFN_one_iter(model, data_loader, optimizer=None):\n",
    "    assert model.__class__.__name__ == 'TSAFN', 'this TSAFN_one_iter only works for TSAFN model' \n",
    "    \n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss_sum = 0\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            inpu = torch.cat((x['input'].to(device), x['TPN'].to(device), x['SPN'].to(device)), 1)\n",
    "            out = model(inpu)\n",
    "            loss = criterion(out, x['TSAFN'].to(device)).to(device)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def three_to_one(model_dict, data_loader, optimizer=None):\n",
    "    TPN = model_dict['TPN']\n",
    "    SPN = model_dict['SPN']\n",
    "    TSAFN = model_dict['TSAFN']\n",
    "\n",
    "    if optimizer:\n",
    "        is_train = True\n",
    "    else:\n",
    "        is_train = False\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss_sum = 0\n",
    "    for x in data_loader:\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            inpu = x['input'].to(device)\n",
    "            target_TPN = x['TPN'].to(device)\n",
    "            target_SPN = x['SPN'].to(device)\n",
    "            target_TSAFN = x['TSAFN'].to(device)\n",
    "            \n",
    "            output_TPN = TPN(inpu)\n",
    "            output_SPN = SPN(inpu)\n",
    "            concat = torch.cat((inpu, output_TPN, output_SPN[-1]), 1) # what really counts for SPN is fusion output \n",
    "            out = TSAFN(concat)\n",
    "            \n",
    "            loss_SPN = SPN_loss(*output_SPN, target_SPN)\n",
    "            loss_TPN = F.mse_loss(output_TPN, target_TPN)\n",
    "            loss_TSAFN = F.mse_loss(out, target_TSAFN)\n",
    "            print(loss_SPN)\n",
    "            print(loss_TPN)\n",
    "            print(loss_TSAFN)\n",
    "            loss = 0.6 * loss_TSAFN + 0.2 * (loss_SPN + loss_TPN)\n",
    "            loss = loss.to(device)\n",
    "            print(loss)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        loss_sum += loss\n",
    "    epoch_loss = loss_sum / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "    \n",
    "\n",
    "def train(model, model_iter, train_loader, val_loader, optimizer=None, epochs=500, save_interval=2):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_time = time.time()\n",
    "        train_loss = model_iter(model, train_loader, optimizer)\n",
    "        val_loss = model_iter(model, val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_time\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(\"Epoch {} finished, current loss is {} takes {}\".format(epoch + 1, val_loss, \n",
    "                                                                          timeformat(epoch_time)))\n",
    "            print('It takes {} from beginning'.format(timeformat(time.time() - start_time)))\n",
    "            \n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            torch.save(model, '{}epoch_result'.format(epoch + 1))\n",
    "        \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def timeformat(s):\n",
    "    s = int(s)\n",
    "    m, s = divmod(s, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    t = '{:>02d}:{:>02d}:{:>02d}'.format(h, m, s) if h else '{:>02d}:{:>02d}'.format(m, s) if m else '{:2d}s'.format(s)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SPN(nn.Module):\n",
    "    def __init__(self, body_model='vgg11'):\n",
    "        super(SPN, self).__init__()\n",
    "        \n",
    "        self.body = VGGNet(model=body_model)\n",
    "        \n",
    "        self.side_output_layer1 = nn.Conv2d(64, 1, 1)\n",
    "        self.side_output_layer2 = nn.Conv2d(128, 1, 1)\n",
    "        self.side_output_layer3 = nn.Conv2d(256, 1, 1)\n",
    "        self.side_output_layer4 = nn.Conv2d(512, 1, 1)\n",
    "        self.side_output_layer5 = nn.Conv2d(512, 1, 1)\n",
    "        \n",
    "        self.fusion = nn.Conv2d(5, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, h, w = x.size()\n",
    "        \n",
    "        # load vgg\n",
    "        body = self.body(x)\n",
    "\n",
    "        # side-output layers\n",
    "        side_output1 = body['side_output1']\n",
    "        side_output1 = self.side_output_layer1(side_output1)\n",
    "        side_output1 = F.interpolate(side_output1, size=(h, w))\n",
    "        \n",
    "        side_output2 = body['side_output2']\n",
    "        side_output2 = self.side_output_layer2(side_output2)\n",
    "        side_output2 = F.interpolate(side_output2, size=(h,w))\n",
    "        \n",
    "        side_output3 = body['side_output3']\n",
    "        side_output3 = self.side_output_layer3(side_output3)\n",
    "        side_output3 = F.interpolate(side_output3, size=(h,w))\n",
    "        \n",
    "        side_output4 = body['side_output4']\n",
    "        side_output4 = self.side_output_layer4(side_output4)\n",
    "        side_output4 = F.interpolate(side_output4, size=(h,w))\n",
    "        \n",
    "        side_output5 = body['side_output5']\n",
    "        side_output5 = self.side_output_layer5(side_output5)\n",
    "        side_output5 = F.interpolate(side_output5, size=(h,w))\n",
    "        \n",
    "        # fusion layer\n",
    "        fuse = torch.cat((side_output1, side_output2, side_output3, side_output4, side_output5), 1)\n",
    "        fusion = self.fusion(fuse)\n",
    "\n",
    "        # will use cross_entropy_with_logit loss, so no need apply sigmoid on the final output.\n",
    "        return side_output1, side_output2, side_output3, side_output4, side_output5, fusion\n",
    "\n",
    "\n",
    "# the strategy to both get the structure and pretrained weight of in-built vgg\n",
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg11', requires_grad=True):\n",
    "  \n",
    "        super().__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.{}_bn(pretrained=True).state_dict())\".format(model))\n",
    "        \n",
    "        # for fix weight transfor learning\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # first reconstructure same vgg structure as in-bulit to utilize pretraining, but del the redundent fc later\n",
    "        del self.classifier  \n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # catch the middle outputs \n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            \n",
    "            output[\"side_output%d\"%(idx+1)] = x\n",
    "\n",
    "        return output\n",
    "\n",
    "# this range only for batchnorm version\n",
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 7),  (7, 14),  (14, 21), (21, 28)),\n",
    "    'vgg13': ((0, 6), (6, 13), (13, 20), (20, 27), (27, 34)),\n",
    "    'vgg16': ((0, 6), (6, 13), (13, 23), (23, 32), (32, 43)),\n",
    "    'vgg19': ((0, 6), (6, 13), (13, 26), (26, 39), (39, 52))\n",
    "}\n",
    "\n",
    "def make_layers(cfg):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmoothData(Dataset):\n",
    "    def __init__(self, path, split_ratio={'train':0.65, 'val':0.1, 'test':0.25}, transform=None):\n",
    "        self.transforms = transform\n",
    "        self.category = {}\n",
    "        self.name = {'input', 'target_SPN', 'target_TPN', 'target_TSAFN'}\n",
    "        self.mode = None\n",
    "        self.modeset = ['train', 'val', 'test']\n",
    "        \n",
    "        check_folder = set()\n",
    "        for directory, folders, _ in os.walk(path):\n",
    "            for f in folders:\n",
    "                self.category[f] = glob.glob(os.path.join(directory, f) + '\\\\*')\n",
    "                check_folder.add(f)\n",
    "            break  # only retrieve one level\n",
    "            \n",
    "        assert not (self.name - check_folder), 'folder name must be same as declared in self.name'\n",
    "        assert sum(split_ratio.values()) == 1, 'the summation of values in split_ratio must be one'\n",
    "\n",
    "        self.total = len(self.category['input'])\n",
    "        \n",
    "        index_shuffle = list(range(self.total))\n",
    "        random.shuffle(index_shuffle)\n",
    "        \n",
    "        train_amount = int(self.total * split_ratio['train'])\n",
    "        val_amount = int(self.total * split_ratio['val'])\n",
    "        self.amount = {'train': train_amount,\n",
    "                       'val': val_amount,\n",
    "                       'test': self.total - train_amount - val_amount}  # must use minus, otherwise not full.         \n",
    "\n",
    "        index_train = index_shuffle[: self.amount['train']]\n",
    "        index_val = index_shuffle[self.amount['train'] : self.amount['train'] + self.amount['val']]\n",
    "        index_test = index_shuffle[self.amount['train'] + self.amount['val'] :]\n",
    "\n",
    "        \n",
    "        # the file retrieval order is not sequentially from 1 to end, but may be like 1, 10, 11, ... , 2, 20 ..\n",
    "        # this part is to align the image index into sequential order for target_SPN and target_TSAFN\n",
    "        target_length = len(self.category['target_SPN'])\n",
    "        temp1 = [0 for i in range(target_length)]\n",
    "        temp2 = copy.deepcopy(temp1)\n",
    "        for i in range(target_length):\n",
    "            img_path = self.category['target_SPN'][i]\n",
    "            _, img_name = os.path.split(img_path)\n",
    "            img_index = int(img_name.split('.')[0])\n",
    "            temp1[img_index] = img_path\n",
    "            \n",
    "            img_path = self.category['target_TSAFN'][i]\n",
    "            _, img_name = os.path.split(img_path)\n",
    "            img_index = int(img_name.split('.')[0])\n",
    "            temp2[img_index] = img_path\n",
    "        self.category['target_SPN'] = temp1\n",
    "        self.category['target_TSAFN'] = temp2\n",
    "        \n",
    "        self.collection = {'train':([[0 for i in range(4)] for i in range(self.amount['train'])], index_train),\n",
    "                           'val':([[0 for i in range(4)] for i in range(self.amount['val'])], index_val),\n",
    "                           'test':([[0 for i in range(4)] for i in range(self.amount['test'])], index_test)}\n",
    "        \n",
    "        for m in self.modeset:\n",
    "            for subset_index, alldata_index in enumerate(self.collection[m][1]):\n",
    "                _, img_name = os.path.split(self.category['input'][alldata_index])\n",
    "                img_index = int(img_name.split('_')[0])\n",
    "\n",
    "                self.collection[m][0][subset_index] = [self.category['input'][alldata_index], \n",
    "                                      self.category['target_TPN'][alldata_index], \n",
    "                                      self.category['target_SPN'][img_index], \n",
    "                                      self.category['target_TSAFN'][img_index]]  \n",
    "\n",
    "    def __call__(self, mode):\n",
    "        assert mode in self.modeset, 'mode must be either train, val or test'\n",
    "        self.mode = mode\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.mode:\n",
    "            return self.amount[self.mode]\n",
    "        else:\n",
    "            raise Exception('Firstly, you must use self.setmode(mode) to set which data set (train, val, test) to be use.')\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        # check if mode is set\n",
    "        len(self)\n",
    "       \n",
    "        dataset = self.collection[self.mode][0]\n",
    "        inpu = Image.open(dataset[i][0])\n",
    "        target_TPN = Image.open(dataset[i][1])\n",
    "        target_SPN = Image.open(dataset[i][2])\n",
    "        target_TSAFN = Image.open(dataset[i][3])\n",
    "        \n",
    "        if self.transforms:\n",
    "            inpu = self.transforms(inpu)\n",
    "            target_TPN = self.transforms(target_TPN)\n",
    "            target_SPN = self.transforms(target_SPN)\n",
    "            target_TSAFN = self.transforms(target_TSAFN)\n",
    "\n",
    "        return {'input':inpu, 'TPN':target_TPN, 'SPN':target_SPN, 'TSAFN':target_TSAFN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "data = SmoothData('../verysmall_dataset', transform=transform)  \n",
    "\n",
    "#train_sampler = sampler.SubsetRandomSampler(range(18000))\n",
    "#val_sampler = sampler.SubsetRandomSampler(range(18000, 20000))\n",
    "\n",
    "train_loader = DataLoader(data('train'), batch_size=2)\n",
    "val_loader = DataLoader(data('val'), batch_size=10)\n",
    "test_loader = DataLoader(data('test'), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programming_tools\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "D:\\programming_tools\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "from skimage import io, img_as_float, img_as_uint, transform\n",
    "img1 = img_as_float(io.imread('../verysmall_dataset/input/4_0.jpg'))\n",
    "img2 = img_as_float(transform.resize(io.imread('../verysmall_dataset/target_TSAFN/4.jpg'), (400, 400, 3)))\n",
    "\n",
    "img = abs(img1 - img2)\n",
    "img = img.sum(2)\n",
    "\n",
    "io.imsave('tanhtimes.jpg', np.tanh(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2,3,3,3)\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2266e633550>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import utils \n",
    "img = data('train')[-1]['SPN']\n",
    "img = img.expand(3,-1,-1).transpose(0, 2).transpose(0,1)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2999, grad_fn=<ThAddBackward>)\n",
      "tensor(0.1707, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1392, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3776, grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-670-bff527675e2b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, model_iter, train_loader, val_loader, optimizer, epochs, save_interval)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0mepoch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-670-bff527675e2b>\u001b[0m in \u001b[0;36mthree_to_one\u001b[1;34m(model_dict, data_loader, optimizer)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming_tools\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming_tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dict = {'SPN': SPN().to(device), 'TPN': TPN().to(device), 'TSAFN': TSAFN().to(device)} \n",
    "#model = torch.load('9epoch_result')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "loss, val_losses = train(model_dict, three_to_one, train_loader, val_loader, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-c9a5400bffed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSmoothData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\smooth_net\\data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#c = utils.make_grid(torch.cat((a[0], a[1], a[2], a[3]), 2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-285cd051f099>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mtarget_TSAFN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_TPN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_TPN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "a = SmoothData('D:\\smooth_net\\data', transform=transform)[3]\n",
    "#c = utils.make_grid(torch.cat((a[0], a[1], a[2], a[3]), 2))\n",
    "c1 = a[0].numpy().transpose((1,2,0))\n",
    "c2 = a[1].numpy().transpose((1,2,0))\n",
    "c3 = a[2].numpy().transpose((1,2,0))\n",
    "c4 = a[3].numpy().transpose((1,2,0))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(141)\n",
    "plt.imshow(c1)\n",
    "plt.subplot(142)\n",
    "plt.imshow(c2[:,:,0], cmap='gray')\n",
    "plt.subplot(143)\n",
    "plt.imshow(c3[:,:,0], cmap='gray')\n",
    "plt.subplot(144)\n",
    "plt.imshow(c4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
